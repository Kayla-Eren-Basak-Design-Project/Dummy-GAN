{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15fef83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aeb5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAP_SIZE = 32\n",
    "LR_D = LR_G = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09eca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "N = 32\n",
    "\n",
    "def generate_rect(x, y, w, h):\n",
    "  data = np.zeros((N,N))\n",
    "  data[x:x+w+1, y:y+h+1] = 1\n",
    "  data[x+1:x+w, y+1:y+h] = 0\n",
    "  #data = np.reshape(data, (N*N))\n",
    "  return data\n",
    "\n",
    "train_images = []\n",
    "\n",
    "for x in range(0, N-2):\n",
    "  for y in range(0, N-2):\n",
    "    for w in range(2,N):\n",
    "      for h in range(2,N):\n",
    "        if x+w<N and y+h<N:\n",
    "          train_images.append(generate_rect(x,y,w,h))\n",
    "train_images = np.array(train_images).astype('int32')\n",
    "train_images = train_images[np.random.choice(np.shape(train_images)[0], 1000, replace=False)]\n",
    "\n",
    "print(np.shape(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73245b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d9d0a79d80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46488af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(1000, 32, 32, 2)\n",
      "(1000, 32, 32, 2)\n",
      "(1000, 32, 32, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size, x_dims, y_dims = train_images.shape\n",
    "z_dims = np.amax(train_images) + 1 # Number of different title types\n",
    "print(z_dims)\n",
    "train_images_onehot = np.eye(z_dims, dtype='uint8')[train_images]\n",
    "print(train_images_onehot.shape) # (train_size, x_dims, y_dims, z_dims)\n",
    "\n",
    "train_images = np.zeros((train_size, MAP_SIZE, MAP_SIZE, z_dims))\n",
    "print(train_images.shape)\n",
    "\n",
    "# TODO: Change empty space encoding here if different\n",
    "train_images[:, :, :, 1] = 0.0  # Fill with empty space \n",
    "\n",
    "train_images[:train_size, :x_dims, :y_dims, :] = train_images_onehot\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86c5543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2da2e109120>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.argmax(train_images[2], axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeba46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_images.shape[0]).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c346ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(n_extra_layers=0):\n",
    "  assert MAP_SIZE % 16 == 0\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', use_bias=False, input_shape=(MAP_SIZE, MAP_SIZE, z_dims)))\n",
    "  model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "  image_size, n_filters = MAP_SIZE / 2, 64\n",
    "\n",
    "  # Extra layers\n",
    "  for i in range(n_extra_layers):\n",
    "    model.add(layers.Conv2D(n_filters, (3, 3), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "  while image_size > 4:\n",
    "    n_filters *= 2\n",
    "    model.add(layers.Conv2D(n_filters, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    image_size /= 2\n",
    "\n",
    "  # Input here is (BATCH_SIZE x 4 x 4 x n_filters)\n",
    "  model.add(layers.Conv2D(1, (4, 4), strides=(1, 1), padding='valid', use_bias=False))\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a32547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(n_extra_layers=0):\n",
    "  assert MAP_SIZE % 16 == 0\n",
    "\n",
    "  noise_size = 128\n",
    "  # 32 here is the 1/2 * n_channels before last Conv2DTranspose\n",
    "  n_filters = 32 * MAP_SIZE / 4\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.Input(shape=(noise_size, )))\n",
    "  model.add(layers.Reshape((1, 1, noise_size)))\n",
    "  \n",
    "  model.add(layers.Conv2DTranspose(n_filters, (4, 4), strides=(1, 1), padding='valid', use_bias=False))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.ReLU())\n",
    "\n",
    "  image_size = 4\n",
    "  while image_size < MAP_SIZE / 2:\n",
    "    n_filters /= 2\n",
    "    model.add(layers.Conv2DTranspose(n_filters, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    image_size *= 2\n",
    "\n",
    "  # Extra layers\n",
    "  for i in range(n_extra_layers):\n",
    "    model.add(layers.Conv2DTranspose(n_filters, (3, 3), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "  model.add(layers.Conv2DTranspose(z_dims, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "  model.add(layers.ReLU())\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc2df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c002f889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 64)        2048      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 128)         131072    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8, 8, 128)        512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 256)         524288    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 1)           4096      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 663,040\n",
      "Trainable params: 662,272\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 4, 4, 256)        524288    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 128)        524288    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 64)       131072    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 32, 32, 2)        2048      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 32, 32, 2)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,183,488\n",
      "Trainable params: 1,182,592\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(discriminator.summary())\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e65f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 2)\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(generator(np.random.rand(1, 128)).shape)\n",
    "print(discriminator(np.random.rand(1, MAP_SIZE, MAP_SIZE, z_dims)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a74a6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(real_output, generated_output):\n",
    "  real_loss = tf.reduce_mean(real_output)\n",
    "  generated_loss = tf.reduce_mean(generated_output)\n",
    "  total_loss = real_loss - generated_loss\n",
    "\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "def get_generator_loss(generated_output):\n",
    "  return -tf.reduce_mean(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62374aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = tf.optimizers.RMSprop(LR_D)\n",
    "generator_optimizer = tf.optimizers.RMSprop(LR_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc2a3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, savefig=False):\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(np.argmax(predictions[i], axis=2))\n",
    "    plt.axis('off')\n",
    "\n",
    "  if savefig:\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae9c3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "noise_size = 128\n",
    "num_examples = 8\n",
    "\n",
    "# Use same random vector to see evolution of generated images over time\n",
    "random_vector_for_generation = tf.random.normal([num_examples, noise_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e63ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, update_generator=False):\n",
    "  # Generating noise from a normal distribution\n",
    "  noise = tf.random.normal([BATCH_SIZE, noise_size])\n",
    "\n",
    "  for w in discriminator.trainable_variables:\n",
    "    w.assign(tf.clip_by_value(w, -0.01, 0.01))\n",
    "    \n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "    real_output = discriminator(images, training=True)\n",
    "    generated_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    gen_loss = get_generator_loss(generated_output)\n",
    "    disc_loss = get_discriminator_loss(real_output, generated_output)\n",
    "        \n",
    "    gradients_of_discriminator = disc_tape.gradient(-disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    if update_generator:\n",
    "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "  return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88574fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca638373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  gen_iterations = 0\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    disc_iterations = 0\n",
    "    for i, images in enumerate(dataset):\n",
    "      if disc_iterations == 0:\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "          disc_iterations = 100\n",
    "        else:\n",
    "          disc_iterations = 5\n",
    "      if disc_iterations == 1 or i == len(dataset) - 1:\n",
    "        gen_loss, disc_loss = train_step(images, True)\n",
    "        gen_iterations += 1\n",
    "      else:\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "      disc_iterations -= 1\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    # Save generator every 500 epochs\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "      generate_and_save_images(generator, epoch + 1, \n",
    "                               random_vector_for_generation, True)\n",
    "      generator.save('models/generator_baseline_' + str(epoch + 1))\n",
    "    else:\n",
    "      generate_and_save_images(generator, epoch + 1, \n",
    "                               random_vector_for_generation)\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec'.format(epoch + 1, \n",
    "                                                      time.time() - start))\n",
    "    print(\"Generator Loss: \", gen_loss)\n",
    "    print(\"Discriminator Loss: \", disc_loss)\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                          epochs,\n",
    "                          random_vector_for_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4dc458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch 66 is 4.884673595428467 sec\n",
      "Generator Loss:  tf.Tensor(0.1261532, shape=(), dtype=float32)\n",
      "Discriminator Loss:  tf.Tensor(0.28538254, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e7e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
